{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 5)\n",
      "(5572, 2)\n"
     ]
    }
   ],
   "source": [
    "#read data into dataframe\n",
    "\n",
    "data = pd.read_csv('spam ham data set.csv', encoding = \"ISO-8859-1\")\n",
    "print(data.shape)\n",
    "data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis=1, inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the files if required.\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will Ì_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text to lowercase\n",
    "\n",
    "def to_lower(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove digits from the data\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub(r'\\d+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuations marks and special symbol\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('','', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    tex = ''\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    for i in tokens:\n",
    "        if not i in stop_words:\n",
    "            tex = tex +\" \"+ i\n",
    "    return tex\n",
    "#     return [i for i in tokens if not i in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the data\n",
    "\n",
    "def preprocess(data):\n",
    "    lower_data = data.applymap(to_lower)\n",
    "    rem_digit_data = lower_data.applymap(remove_digits)\n",
    "    no_punc_data = rem_digit_data.applymap(remove_punctuation)\n",
    "    rem_stopwords = no_punc_data.applymap(remove_stop_words)\n",
    "    \n",
    "    return rem_stopwords.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marking spam as 1 and ham as 0 after preprocessing\n",
    "\n",
    "clean_data = preprocess(data)\n",
    "clean_data['v1'] = clean_data['v1'].replace({'spam':1, 'ham':0}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4059 1015\n"
     ]
    }
   ],
   "source": [
    "#splitting data into testing and training set\n",
    "\n",
    "train, test = train_test_split(clean_data, test_size = 0.20)\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting the number of spam and ham in training set\n",
    "\n",
    "count_spam = train.groupby('v1').count()['v2'].array[1]\n",
    "count_ham = train.groupby('v1').count()['v2'].array[0]\n",
    "total = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dictionaries and word counts\n",
    "\n",
    "dictionary = {}\n",
    "word_dict = {}\n",
    "word_no = -1\n",
    "\n",
    "count_spam_words = 0\n",
    "count_ham_words = 0\n",
    "\n",
    "for i in range(total):\n",
    "    s_h = train['v1'].iloc[i]\n",
    "    words_line = train['v2'].iloc[i].split()\n",
    "    for word in words_line:\n",
    "        \n",
    "        if word not in word_dict:\n",
    "            word_no += 1\n",
    "            word_dict[word] = word_no\n",
    "        \n",
    "        if s_h == 0:\n",
    "            try:\n",
    "                dictionary[word][0] += 1\n",
    "            except KeyError as e:\n",
    "                count_ham_words += 1\n",
    "                dictionary[word] = [1, 0]    \n",
    "        \n",
    "        elif s_h == 1:\n",
    "            try:\n",
    "                dictionary[word][1] += 1\n",
    "            except KeyError as e:\n",
    "                count_spam_words += 1\n",
    "                dictionary[word] = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the probability of different words\n",
    "\n",
    "for item in list(dictionary):\n",
    "    dictionary[item][0] = (dictionary[item][0] + 1) / (count_ham_words + len(word_dict))\n",
    "    dictionary[item][1] = (dictionary[item][1] + 1) / (count_spam_words + len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the vector for a given sentence\n",
    "\n",
    "def vectorize(sentence, word_dict):\n",
    "    vector = [0] * len(word_dict)\n",
    "    for word in sentence:\n",
    "        if word in word_dict:\n",
    "            index = word_dict[word]\n",
    "            vector[index] += 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probability of spam and ham in training data\n",
    "\n",
    "prob_spam = count_spam/total\n",
    "prob_ham = count_ham/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting if the mails in test set are spam or ham using the calculated probabilities\n",
    "\n",
    "pred_ham = 0\n",
    "pred_spam = 0\n",
    "\n",
    "pred_correct = 0\n",
    "\n",
    "word_list = list(dictionary.keys())\n",
    "\n",
    "for i in range(len(test)):\n",
    "    prob_data_spam = 1.0\n",
    "    prob_data_ham = 1.0\n",
    "    \n",
    "    words_line = test['v2'].iloc[i].split()\n",
    "    correct_output = test['v1'].iloc[i]\n",
    "    \n",
    "    vector = vectorize(words_line, word_dict)\n",
    "    \n",
    "    for i,j in enumerate(vector):\n",
    "        word = word_list[i]\n",
    "        if j == 0:\n",
    "            prob_data_spam *= (1 - dictionary[word][1])\n",
    "            prob_data_ham *= (1 - dictionary[word][0])\n",
    "        elif j == 1:\n",
    "            prob_data_spam *= dictionary[word][1]\n",
    "            prob_data_ham *= dictionary[word][0]\n",
    "\n",
    "    total_prob = (prob_ham * prob_data_ham) + (prob_spam * prob_data_spam)\n",
    "\n",
    "    prob_ham_data = (prob_data_ham * prob_ham)/ total_prob\n",
    "    prob_spam_data = (prob_data_spam * prob_spam)/ total_prob\n",
    "    \n",
    "    if prob_ham_data > prob_spam_data:\n",
    "        pred_ham += 1\n",
    "        if correct_output == 0:\n",
    "            pred_correct += 1\n",
    "    else :\n",
    "        pred_spam += 1\n",
    "        if correct_output == 1:\n",
    "            pred_correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is 97.44 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (pred_correct/len(test))*100\n",
    "print(\"The accuracy of the model is {0:.2f} %\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam :  122  detected :  106\n",
      "Ham  :  893 detected :  909\n"
     ]
    }
   ],
   "source": [
    "act_ham = test.groupby('v1').count()['v2'].array[0]\n",
    "act_spam = test.groupby('v1').count()['v2'].array[1]\n",
    "print(\"Spam : \", act_spam,\" detected : \", pred_spam)\n",
    "print(\"Ham  : \", act_ham,\"detected : \", pred_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
